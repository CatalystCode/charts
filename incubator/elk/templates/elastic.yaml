apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{.Values.Elastic.name}}
---
apiVersion: v1
kind: Service
metadata:
  name: {{.Values.Elastic.name}}
  labels:
    app: {{.Values.Elastic.name}}
spec:
  ports:
  - port: {{.Values.Elastic.port}}
    name: {{.Values.Elastic.port_name}}
  clusterIP: None
  selector:
    name: {{.Values.Elastic.port_selector}}
---
apiVersion: v1
kind: Service
metadata:
  name: {{.Values.Elastic.cluster_name}}
  labels:
    app: {{.Values.Elastic.cluster_name}}
spec:
  ports:
  - port: {{.Values.Elastic.cluster_port}}
    name: {{.Values.Elastic.cluster_port_name}}
  clusterIP: None
  selector:
    app: {{.Values.Elastic.name}}
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {{.Values.Elastic.master_name}}
spec:
  serviceName: "{{.Values.master_name}}"
  replicas: {{.Values.master_replicas}}
  template:
    metadata:
      labels:
        app: {{.Values.Elastic.name}}
        name: {{.Values.Elastic.master_name}}
      annotations:
        pod.alpha.kubernetes.io/init-containers: '[
            {
                "name": "max-map-count-set",
                "image": "quay.io/samsung_cnct/set_max_map_count:1.1",
                "volumeMounts": [
                    {
                        "name": "hostproc",
                        "mountPath": "/hostproc"
                    }
                ]
            }]'
    spec:
      serviceAccount: {{.Values.Elastic.name}}
      serviceAccountName: {{.Values.Elastic.name}}
      imagePullSecrets:
        - name: {{.Values.Elastic.image_pull_secret}}
      containers:
      - name: {{.Values.Elastic.name}}
        image: {{.Values.Elastic.image}}
        ports:
        - name: {{.Values.Elastic.cluster_port_name}}
          containerPort: {{.Values.Elastic.cluster_port}}
        #  resource notes:  master is light weight so half a CPU.  Less then 4GB causes the container to OOM
        resources:
          limits:
            cpu: {{.Values.Elastic.master_cpu_limits}}
            memory: {{.Values.Elastic.master_memory_limits}}
          requests:
            cpu: {{.Values.Elastic.master_cpu_requests}}
            memory: {{.Values.Elastic.master_memory_requests}}
        env:
        - name: CLUSTER_NAME
          value: "{{.Values.Elastic.cluster_name}}"
        - name: NODE_DATA
          value: "false"
        - name: NODE_MASTER
          value: "true"
        - name: SERVICE
          value: "{{.Values.Elastic.cluster_name}}"
        #  the min/max *must* be the same due to elasticsearch 5.0 bootstrap checks
        - name: ES_JAVA_OPTS
          value: "-Xms{{ .Values.Elastic.master_Xms_Xmx }} -Xmx{{ .Values.Elastic.master_Xms_Xmx }} -Djava.net.preferIPv4Stack=true"
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        imagePullPolicy: {{.Values.Elastic.master_imagePullPolicy}}
        volumeMounts:
        - name: esdata
          mountPath: /usr/share/elasticsearch/data
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
  volumeClaimTemplates:
  - metadata:
      name: esdata
      annotations:
        volume.alpha.kubernetes.io/storage-class: fast
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: {{.Values.Elastic.master_volume_storage}}
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {{.Values.Elastic.data_name}}
spec:
  serviceName: "{{.Values.Elastic.data_name}}"
  replicas: {{.Values.Elastic.data_replicas}}
  template:
    metadata:
      labels:
        app: {{.Values.Elastic.name}}
        name: {{.Values.Elastic.data_name}}
      annotations:
        pod.alpha.kubernetes.io/init-containers: '[
            {
                "name": "max-map-count-set",
                "image": "quay.io/samsung_cnct/set_max_map_count:1.1",
                "volumeMounts": [
                    {
                        "name": "hostproc",
                        "mountPath": "/hostproc"
                    }
                ]
            }]'
    spec:
      serviceAccount: {{.Values.Elastic.name}}
      serviceAccountName: {{.Values.Elastic.name}}
      imagePullSecrets:
        - name: {{.Values.Elastic.image_pull_secret}}
      containers:
      - name: {{.Values.Elastic.name}}
        image: {{.Values.Elastic.image}}
        ports:
        - name: {{.Values.Elastic.cluster_port_name}}
          containerPort: {{.Values.Elastic.cluster_port}}
        - name: {{.Values.Elastic.port_name}}
          containerPort: {{.Values.Elastic.port}}
        resources:
          limits:
            cpu: {{.Values.Elastic.data_cpu_limits}}
            memory: {{.Values.Elastic.data_memory_limits}}
          requests:
            cpu: {{.Values.Elastic.data_cpu_requests}}
            memory: {{.Values.Elastic.data_memory_requests}}
        env:
        - name: CLUSTER_NAME
          value: "{{ .Values.Elastic.cluster_name }}"
        - name: NODE_DATA
          value: "true"
        - name: NODE_MASTER
          value: "false"
        - name: SERVICE
          value: "{{ .Values.Elastic.cluster_name }}"
        #  the min/max *must* be the same due to elasticsearch 5.0 bootstrap checks.  For a production settings
        #  this should be 16GB.  Don't go over 32GB as the JVM starts to have issues.
        - name: ES_JAVA_OPTS
          value: "-Xms{{ .Values.Elastic.data_Xms_Xmx }} -Xmx{{ .Values.Elastic.data_Xms_Xmx }} -Djava.net.preferIPv4Stack=true"
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        imagePullPolicy: Always
        volumeMounts:
        - name: esdata
          mountPath: /usr/share/elasticsearch/data
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
  volumeClaimTemplates:
  - metadata:
      name: esdata
      annotations:
        volume.alpha.kubernetes.io/storage-class: fast
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          #  set to small so when exploring with this the user doesn't accidentally create very large disks
          storage: {{.Values.Elastic.data_volume_storage}}
